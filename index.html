
<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

  <title>PoseProbe</title>
</head>

<body>
  <div class="container">
    <br>
    <div style="text-align: center;">  
      <h1>PoseProbe</h1>
      <h3>Generic Objects as Pose Probes for Few-Shot View Synthesis</h3>
      <div style="margin-top: 15px;">
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://github.com/zhirui-gao/" target="_blank">Zhirui Gao</a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://zhirui-gao.github.io/PoseProbe.github.io/" target="_blank">Renjiao Yi</a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://zhirui-gao.github.io/PoseProbe.github.io/" target="_blank">Chenyang Zhu</a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://zhirui-gao.github.io/PoseProbe.github.io/" target="_blank">Ke Zhuang</a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://zhirui-gao.github.io/PoseProbe.github.io/" target="_blank">Wei Cheng</a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://kevinkaixu.net/" target="_blank">Kai Xu</a><sup>*</sup></span
      </div>
      <div>
        <span style="margin-right: 10px; font-size: 1.3em;">National University of Defense Technology</span>
      </div>
    </div>

    <div class="text-center" style="font-size: 1.5em; margin-top: 25px;">
      <a class="btn btn-primary btn-lg" target="_blank"
        href="https://arxiv.org/pdf/2408.16690" role="button"
        style="margin-right: 10px;  margin-bottom: 10px;">Arxiv</a>
        <a class="btn btn-primary btn-lg" target="_blank"
        href="https://zhirui-gao.github.io/PoseProbe.github.io/" role="button"
        style="margin-right: 10px;  margin-bottom: 10px;">Code</a>
        <a class="btn btn-primary btn-lg" target="_blank"
        href="https://zhirui-gao.github.io/PoseProbe.github.io/" role="button"
        style="margin-right: 10px;  margin-bottom: 10px;">Video</a>
       
    </div>

    <div class="row">
      <div class="col-md-12 col-sm-12 col-xs-12">
	<div class="embed-responsive embed-responsive-21by9">
          <video controls loop muted autoplay class="embed-responsive-item">
            <source src="vedio_results.mp4" type="video/mp4">
          </video>
        </div>
        <p class="text-center" style="font-size: 1.5em;">
          <span style="font-weight: bold;">PoseProbe</span> enables realistic view synthesis from few input
          images <span style="font-weight: bold;"> without pose prior</span>.
        </p>
      </div>
    </div>

    <div style="margin-top: 30px;">
      <h2 class="text-center">
        Abstract
      </h2>
      <p style="font-style: italic; margin-bottom: 5px;" class="text-left">
        Radiance fields including NeRFs and 3D Gaussians demonstrate great potential in high-fidelity rendering and scene
        reconstruction, while they require a substantial number of posed images as inputs. COLMAP is frequently employed
        for preprocessing to estimate poses, while it necessitates a large number of feature matches to operate effectively, and it
        struggles with scenes characterized by sparse features, large baselines between images, or a limited number of input images. 
        We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images. Traditional methods often
        use calibration boards but they are not common in images. We propose a novel idea of utilizing everyday objects, 
        commonly found in both images and real life, as “pose probes”. The probe object is automatically segmented by SAM, 
        whose shape is initialized from a cube. We apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. Specifically, object poses of two views are first estimated
        by PnP matching in an SDF representation, which serves as initial poses. PnP matching, requiring only a few features, is suitable for feature-sparse scenes. Additional views
        are incrementally incorporated to refine poses from preceding views. In experiments, PoseProbe achieves state-of-theart performance in both pose estimation and novel view synthesis across multiple datasets. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes
        where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance.
      </p>
  
    </div>


    

    <div style="margin-top: 50px;">
      <div class="text-center">
        <h2>
          Method Overview
        </h2>
        <img src="method_fig.png" width=100% class="img-fluid" alt="Responsive image">
      </div>
      <div style="margin-top: 35px;" class="text-left">
        <p>
          We leverage  <span style="font-weight: bold;"></span>generic objects</span> <span style="font-weight: bold;">in few-view input images</span> 
          as <span style="font-weight: bold;">pose probes</span>. The pose probe is automatically segmented by SAM with prompts, 
          and initialized by <span style="font-weight: bold;">a cube shape</span>. The method does not introduce extra burden but
          successfully facilitates pose estimation in feature-sparse scenes. 
        </p>
      </div>
    </div>

    <div style="margin-top:50px;">
      <h2 class="text-center">
        Results
      </h2>
      <h4 class="text-left" >View Synthesis on our synthesis dataset from 3 Input Views</h4>
      <p class="text-left">
       This dataset is generated using BlenderProc with wide-baseline views. 
        We only have access to <span style="font-weight: bold;">3 input views</span>, without <span style="font-weight: bold;"> camera pose prior</span>. 
        <span style="font-weight: bold;">Our method renders clearer details and fewer artifacts compared to other pose-free baselines!</span> Note that the
        camera poses derived via PnP in our method serve as the initial poses for all NeRF baseline for a fair comparison.
      </p>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="videos/subset_3/hat.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="videos/subset_3/coke.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="videos/subset_3/deskcar.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
     

      <h4  class="text-left" >View Synthesis on DTU from 3 Input Views with Noisy Camera Poses </h4>
      <p class="text-left" >
        DTU is composed of complex object-centric scenes, with wide-baseline views spanning a half-hemisphere.
        We only have access to <span style="font-weight: bold;">3 input views</span>, <span style="font-weight: bold;">without camera pose prior</span>.  
        As before,  All baselines suffer from blurriness and inaccurate scene geometry, while our approach produces much better-quality novel-view renderings thanks to the
        pose probe constraint.  Note that the camera poses derived via PnP in our method serve as the initial poses for all NeRF baseline for a fair comparison.
      </p>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="videos/subset_3/scan1.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="videos/subset_3/scan2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div class="embed-responsive embed-responsive-21by9">
            <video controls loop muted autoplay class="embed-responsive-item">
              <source src="videos/subset_3/scan4.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>

      

      <div>
        <h2 class="text-center" style="margin-top: 30px;">
          Citation
        </h2>
        <p class="text-left">
          If you want to cite our work, please use:
        </p>
        <pre class="text-left">
          @article{gao2024generic,
            title={Generic Objects as Pose Probes for Few-Shot View Synthesis},
            author={Gao, Zhirui and Yi, Renjiao and Zhu, Chenyang and Zhuang, Ke and Chen, Wei and Xu, Kai},
            journal={arXiv preprint arXiv:2408.16690},
            year={2024}
          }
      </pre>
      </div>

      <!-- Optional JavaScript -->
      <!-- jQuery first, then Popper.js, then Bootstrap JS -->
      <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
        integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
        crossorigin="anonymous"></script>
      <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
        integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
        crossorigin="anonymous"></script>

      
        <footer class="footer">
          <div class="container">
            <div class="columns is-centered">
              <div class="column is-8">
                <div class="content">
        
                  <p>
                    This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                    You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  </p>
        
                </div>
              </div>
            </div>
          </div>
        </footer>

</body>

</html>

